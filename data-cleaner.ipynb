{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27ce0c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleaning Kaggle Data (Fixed) ===\n",
      "  Resume JSON loaded: 962 records\n",
      "  JSON corrupt: Expecting ',' delimiter: line 9812792 column 257 (char 596467371)\n",
      "\n",
      "  Kaggle cleaning done!\n",
      "  Total records: 962\n",
      "  Saved: data\\cleaned\\kaggle_clean.json\n",
      "    \n",
      "  all_data_clean.json updated: 962 total records\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Kaggle Data Cleaner - Fixed Version\n",
    "Corrupt JSON files ko bhi handle karta hai\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "CLEAN_DIR = Path(\"data/cleaned\")\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s\\.,\\-\\(\\)\\/\\+]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_skills(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    SKILLS = [\n",
    "        \"python\", \"java\", \"javascript\", \"typescript\", \"c++\", \"c#\", \"sql\",\n",
    "        \"react\", \"angular\", \"vue\", \"nodejs\", \"django\", \"flask\", \"fastapi\",\n",
    "        \"machine learning\", \"deep learning\", \"nlp\", \"tensorflow\", \"pytorch\",\n",
    "        \"scikit-learn\", \"pandas\", \"numpy\", \"keras\",\n",
    "        \"aws\", \"azure\", \"gcp\", \"docker\", \"kubernetes\", \"git\",\n",
    "        \"postgresql\", \"mongodb\", \"mysql\", \"redis\", \"elasticsearch\",\n",
    "        \"html\", \"css\", \"rest api\", \"graphql\", \"microservices\",\n",
    "        \"data science\", \"data analysis\", \"computer vision\",\n",
    "        \"linux\", \"agile\", \"scrum\", \"ci/cd\", \"jenkins\",\n",
    "    ]\n",
    "    text_lower = text.lower()\n",
    "    return [s for s in SKILLS if s in text_lower]\n",
    "\n",
    "def extract_experience_years(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    patterns = [\n",
    "        r'(\\d+)\\+?\\s*years?\\s*of\\s*experience',\n",
    "        r'(\\d+)\\+?\\s*years?\\s*experience',\n",
    "        r'experience\\s*of\\s*(\\d+)\\+?\\s*years?',\n",
    "        r'(\\d+)\\+?\\s*yr[s]?\\s*exp',\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        match = re.search(pat, text.lower())\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def clean_kaggle_only():\n",
    "    print(\"=== Cleaning Kaggle Data (Fixed) ===\")\n",
    "    all_records = []\n",
    "\n",
    "    # ---- Resume Screening ----\n",
    "    resume_json = DATA_DIR / \"kaggle\" / \"Resume Screening.json\"\n",
    "    resume_csv  = DATA_DIR / \"kaggle\" / \"Resume Screening.csv\"\n",
    "\n",
    "    if resume_json.exists():\n",
    "        try:\n",
    "            with open(resume_json, encoding=\"utf-8\") as f:\n",
    "                resumes = json.load(f)\n",
    "            print(f\"  Resume JSON loaded: {len(resumes)} records\")\n",
    "        except:\n",
    "            print(\"  Resume JSON corrupt, CSV se load kar raha hoon...\")\n",
    "            df = pd.read_csv(resume_csv, on_bad_lines=\"skip\")\n",
    "            resumes = df.to_dict(orient=\"records\")\n",
    "            print(f\"  Resume CSV loaded: {len(resumes)} records\")\n",
    "    elif resume_csv.exists():\n",
    "        df = pd.read_csv(resume_csv, on_bad_lines=\"skip\")\n",
    "        resumes = df.to_dict(orient=\"records\")\n",
    "        print(f\"  Resume CSV loaded: {len(resumes)} records\")\n",
    "    else:\n",
    "        resumes = []\n",
    "        print(\"  Resume file nahi mili!\")\n",
    "\n",
    "    for r in resumes:\n",
    "        text = r.get(\"resume\", r.get(\"Resume\", r.get(\"resume_text\", r.get(\"text\", \"\"))))\n",
    "        category = r.get(\"category\", r.get(\"Category\", r.get(\"label\", \"\")))\n",
    "        rec = {\n",
    "            \"id\": f\"kaggle_resume_{len(all_records)}\",\n",
    "            \"source\": \"kaggle_resume\",\n",
    "            \"type\": \"resume\",\n",
    "            \"category\": clean_text(str(category)),\n",
    "            \"text\": clean_text(str(text)),\n",
    "            \"skills\": extract_skills(str(text)),\n",
    "            \"experience_years\": extract_experience_years(str(text)),\n",
    "            \"cleaned_at\": datetime.now().isoformat(),\n",
    "        }\n",
    "        if rec[\"text\"]:\n",
    "            all_records.append(rec)\n",
    "\n",
    "    # ---- Job Descriptions ---- (CSV se directly load karo — JSON corrupt hai)\n",
    "    jd_csv = DATA_DIR / \"kaggle\" / \"job_descriptions.csv\"\n",
    "    jd_json = DATA_DIR / \"kaggle\" / \"job_descriptions.json\"\n",
    "\n",
    "    jds = []\n",
    "    if jd_csv.exists():\n",
    "        try:\n",
    "            # Bari file — chunks mein load karo\n",
    "            print(\"  Job Descriptions CSV loading (chunks mein)...\")\n",
    "            chunks = pd.read_csv(jd_csv, on_bad_lines=\"skip\", chunksize=5000, encoding=\"utf-8\")\n",
    "            df_jd = pd.concat(chunks, ignore_index=True)\n",
    "            jds = df_jd.to_dict(orient=\"records\")\n",
    "            print(f\"  Job Descriptions loaded: {len(jds)} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"  CSV error: {e}\")\n",
    "    elif jd_json.exists():\n",
    "        try:\n",
    "            with open(jd_json, encoding=\"utf-8\") as f:\n",
    "                jds = json.load(f)\n",
    "            print(f\"  Job Descriptions JSON loaded: {len(jds)} records\")\n",
    "        except Exception as e:\n",
    "            print(f\"  JSON corrupt: {e}\")\n",
    "\n",
    "    for r in jds:\n",
    "        title = r.get(\"title\", r.get(\"job_title\", r.get(\"position\", r.get(\"Job Title\", \"\"))))\n",
    "        desc  = r.get(\"description\", r.get(\"job_description\", r.get(\"requirements\", r.get(\"Job Description\", \"\"))))\n",
    "        company = r.get(\"company\", r.get(\"company_name\", r.get(\"Company\", \"\")))\n",
    "\n",
    "        rec = {\n",
    "            \"id\": f\"kaggle_jd_{len(all_records)}\",\n",
    "            \"source\": \"kaggle_jd\",\n",
    "            \"type\": \"job_description\",\n",
    "            \"title\": clean_text(str(title)),\n",
    "            \"company\": clean_text(str(company)),\n",
    "            \"description\": clean_text(str(desc)),\n",
    "            \"skills_required\": extract_skills(f\"{title} {desc}\"),\n",
    "            \"experience_years\": extract_experience_years(str(desc)),\n",
    "            \"cleaned_at\": datetime.now().isoformat(),\n",
    "        }\n",
    "        if rec[\"title\"] or rec[\"description\"]:\n",
    "            all_records.append(rec)\n",
    "\n",
    "    # Save\n",
    "    out_path = CLEAN_DIR / \"kaggle_clean.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_records, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\"\"\n",
    "  Kaggle cleaning done!\n",
    "  Total records: {len(all_records)}\n",
    "  Saved: {out_path}\n",
    "    \"\"\")\n",
    "\n",
    "    # all_data_clean.json update karo\n",
    "    all_clean = CLEAN_DIR / \"all_data_clean.json\"\n",
    "    existing = []\n",
    "    if all_clean.exists():\n",
    "        try:\n",
    "            with open(all_clean, encoding=\"utf-8\") as f:\n",
    "                existing = json.load(f)\n",
    "            # Purani kaggle records hata do\n",
    "            existing = [r for r in existing if not r.get(\"source\", \"\").startswith(\"kaggle\")]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    combined = existing + all_records\n",
    "    with open(all_clean, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(combined, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"  all_data_clean.json updated: {len(combined)} total records\")\n",
    "    return all_records\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_kaggle_only()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4ce475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_data_clean.json: 962 records\n",
      "kaggle_clean.json: 962 records\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "for f in Path('data/cleaned').glob('*.json'):\n",
    "    with open(f) as fp:\n",
    "        d = json.load(fp)\n",
    "    print(f'{f.name}: {len(d)} records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f64ca6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indeed: 210 raw -> 198 unique saved\n",
      "linkedin: 306 raw -> 27 unique saved\n",
      "\n",
      "Total merged: 1187 records\n",
      "  Kaggle  : 962\n",
      "  Indeed  : 198\n",
      "  LinkedIn: 27\n",
      "\n",
      "Saved: data/cleaned/all_data_clean.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Indeed + LinkedIn Data Cleaner\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "CLEAN_DIR = Path(\"data/cleaned\")\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def clean_text(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s\\.,\\-\\(\\)\\/\\+]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_skills(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    SKILLS = [\n",
    "        \"python\", \"java\", \"javascript\", \"sql\", \"react\", \"nodejs\",\n",
    "        \"django\", \"flask\", \"machine learning\", \"deep learning\",\n",
    "        \"tensorflow\", \"pytorch\", \"aws\", \"azure\", \"docker\",\n",
    "        \"kubernetes\", \"git\", \"mongodb\", \"mysql\", \"html\", \"css\",\n",
    "        \"data science\", \"linux\", \"typescript\", \"fastapi\", \"numpy\",\n",
    "        \"pandas\", \"scikit-learn\", \"gcp\", \"postgresql\", \"redis\",\n",
    "    ]\n",
    "    return [s for s in SKILLS if s in text.lower()]\n",
    "\n",
    "def clean_source(folder, source_name):\n",
    "    records = []\n",
    "    for f in Path(folder).rglob(\"*.json\"):\n",
    "        if \"all_\" in f.name:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.load(open(f, encoding=\"utf-8\"))\n",
    "            records.extend(data if isinstance(data, list) else [data])\n",
    "        except Exception as e:\n",
    "            print(f\"  Skip {f.name}: {e}\")\n",
    "\n",
    "    cleaned = []\n",
    "    seen = set()\n",
    "    for r in records:\n",
    "        title = clean_text(r.get(\"title\", \"\"))\n",
    "        if not title:\n",
    "            continue\n",
    "        company = clean_text(r.get(\"company\", \"\"))\n",
    "        key = f\"{title.lower()}_{company.lower()}\"\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        desc = clean_text(r.get(\"description\", \"\") or r.get(\"summary\", \"\"))\n",
    "        cleaned.append({\n",
    "            \"id\": f\"{source_name}_{len(cleaned)}\",\n",
    "            \"source\": source_name,\n",
    "            \"title\": title,\n",
    "            \"company\": company,\n",
    "            \"location\": clean_text(r.get(\"location\", \"\")),\n",
    "            \"description\": desc,\n",
    "            \"skills_required\": extract_skills(f\"{title} {desc}\"),\n",
    "            \"scraped_at\": r.get(\"scraped_at\", \"\"),\n",
    "        })\n",
    "\n",
    "    out = CLEAN_DIR / f\"{source_name}_clean.json\"\n",
    "    with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"{source_name}: {len(records)} raw -> {len(cleaned)} unique saved\")\n",
    "    return cleaned\n",
    "\n",
    "# Clean both\n",
    "indeed_data   = clean_source(\"data/indeed\",   \"indeed\")\n",
    "linkedin_data = clean_source(\"data/linkedin\", \"linkedin\")\n",
    "\n",
    "# Merge with kaggle\n",
    "kaggle_path = CLEAN_DIR / \"kaggle_clean.json\"\n",
    "kaggle_data = []\n",
    "if kaggle_path.exists():\n",
    "    kaggle_data = json.load(open(kaggle_path, encoding=\"utf-8\"))\n",
    "\n",
    "all_data = kaggle_data + indeed_data + linkedin_data\n",
    "with open(CLEAN_DIR / \"all_data_clean.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nTotal merged: {len(all_data)} records\")\n",
    "print(f\"  Kaggle  : {len(kaggle_data)}\")\n",
    "print(f\"  Indeed  : {len(indeed_data)}\")\n",
    "print(f\"  LinkedIn: {len(linkedin_data)}\")\n",
    "print(f\"\\nSaved: data/cleaned/all_data_clean.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e241e7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
