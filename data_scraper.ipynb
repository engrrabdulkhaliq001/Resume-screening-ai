{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 09:32:49,613 [INFO] \n",
      "==================================================\n",
      "2026-02-19 09:32:49,615 [INFO] STEP 1: Kaggle Datasets\n",
      "2026-02-19 09:32:49,616 [INFO] ==================================================\n",
      "2026-02-19 09:32:49,618 [INFO] === Kaggle Downloader Start ===\n",
      "2026-02-19 09:32:49,621 [WARNING] Kaggle credentials nahi mile!\n",
      "2026-02-19 09:32:49,623 [INFO] \n",
      "    Kaggle setup karne ke liye:\n",
      "    1. kaggle.com pe account banao\n",
      "    2. Account Settings > API > Create New Token\n",
      "    3. kaggle.json download hoga\n",
      "    4. Yahan copy karo: ~/.kaggle/kaggle.json\n",
      "    5. chmod 600 ~/.kaggle/kaggle.json\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ╔══════════════════════════════════════════╗\n",
      "    ║   Resume Screening AI - Data Scraper    ║\n",
      "    ║   Sources: LinkedIn + Indeed + Kaggle   ║\n",
      "    ╚══════════════════════════════════════════╝\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-19 09:32:54,457 [ERROR] Kaggle credentials nahi hain, skip kar raha hoon\n",
      "2026-02-19 09:32:54,458 [INFO] \n",
      "==================================================\n",
      "2026-02-19 09:32:54,459 [INFO] STEP 2: Indeed Jobs\n",
      "2026-02-19 09:32:54,461 [INFO] ==================================================\n",
      "2026-02-19 09:32:54,462 [INFO] === Indeed Scraper Start ===\n",
      "2026-02-19 09:32:54,466 [INFO] Indeed: Searching 'software engineer'\n",
      "2026-02-19 09:32:55,237 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:32:55,238 [INFO] Indeed Selenium fallback for 'software engineer'\n",
      "2026-02-19 09:33:19,215 [INFO] Indeed 'software engineer': 15 jobs\n",
      "2026-02-19 09:33:19,226 [INFO] Saved: data\\indeed\\software_engineer.json\n",
      "2026-02-19 09:33:28,903 [INFO] Indeed: Searching 'data scientist'\n",
      "2026-02-19 09:33:29,937 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:33:29,939 [INFO] Indeed Selenium fallback for 'data scientist'\n",
      "2026-02-19 09:33:46,431 [INFO] Indeed 'data scientist': 15 jobs\n",
      "2026-02-19 09:33:46,441 [INFO] Saved: data\\indeed\\data_scientist.json\n",
      "2026-02-19 09:33:54,108 [INFO] Indeed: Searching 'machine learning'\n",
      "2026-02-19 09:33:55,466 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:33:55,468 [INFO] Indeed Selenium fallback for 'machine learning'\n",
      "2026-02-19 09:34:11,490 [INFO] Indeed 'machine learning': 15 jobs\n",
      "2026-02-19 09:34:11,498 [INFO] Saved: data\\indeed\\machine_learning.json\n",
      "2026-02-19 09:34:16,663 [INFO] Indeed: Searching 'python developer'\n",
      "2026-02-19 09:34:17,900 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:34:17,903 [INFO] Indeed Selenium fallback for 'python developer'\n",
      "2026-02-19 09:34:34,914 [INFO] Indeed 'python developer': 15 jobs\n",
      "2026-02-19 09:34:34,921 [INFO] Saved: data\\indeed\\python_developer.json\n",
      "2026-02-19 09:34:41,370 [INFO] Indeed: Searching 'backend developer'\n",
      "2026-02-19 09:34:42,917 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:34:42,919 [INFO] Indeed Selenium fallback for 'backend developer'\n",
      "2026-02-19 09:34:59,831 [INFO] Indeed 'backend developer': 15 jobs\n",
      "2026-02-19 09:34:59,853 [INFO] Saved: data\\indeed\\backend_developer.json\n",
      "2026-02-19 09:35:04,982 [INFO] Indeed: Searching 'frontend developer'\n",
      "2026-02-19 09:35:06,046 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:35:06,048 [INFO] Indeed Selenium fallback for 'frontend developer'\n",
      "2026-02-19 09:35:22,255 [INFO] Indeed 'frontend developer': 15 jobs\n",
      "2026-02-19 09:35:22,259 [INFO] Saved: data\\indeed\\frontend_developer.json\n",
      "2026-02-19 09:35:32,109 [INFO] Indeed: Searching 'devops'\n",
      "2026-02-19 09:35:32,936 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:35:32,938 [INFO] Indeed Selenium fallback for 'devops'\n",
      "2026-02-19 09:35:49,594 [INFO] Indeed 'devops': 15 jobs\n",
      "2026-02-19 09:35:49,607 [INFO] Saved: data\\indeed\\devops.json\n",
      "2026-02-19 09:35:58,546 [INFO] Indeed: Searching 'data analyst'\n",
      "2026-02-19 09:36:00,287 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:36:00,288 [INFO] Indeed Selenium fallback for 'data analyst'\n",
      "2026-02-19 09:36:16,768 [INFO] Indeed 'data analyst': 15 jobs\n",
      "2026-02-19 09:36:16,777 [INFO] Saved: data\\indeed\\data_analyst.json\n",
      "2026-02-19 09:36:26,576 [INFO] Indeed: Searching 'full stack developer'\n",
      "2026-02-19 09:36:28,219 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:36:28,221 [INFO] Indeed Selenium fallback for 'full stack developer'\n",
      "2026-02-19 09:36:45,437 [INFO] Indeed 'full stack developer': 15 jobs\n",
      "2026-02-19 09:36:45,443 [INFO] Saved: data\\indeed\\full_stack_developer.json\n",
      "2026-02-19 09:36:53,103 [INFO] Indeed: Searching 'artificial intelligence'\n",
      "2026-02-19 09:36:54,774 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:36:54,776 [INFO] Indeed Selenium fallback for 'artificial intelligence'\n",
      "2026-02-19 09:37:09,162 [INFO] Indeed 'artificial intelligence': 15 jobs\n",
      "2026-02-19 09:37:09,168 [INFO] Saved: data\\indeed\\artificial_intelligence.json\n",
      "2026-02-19 09:37:14,814 [INFO] Indeed: Searching 'cloud engineer'\n",
      "2026-02-19 09:37:16,640 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:37:16,641 [INFO] Indeed Selenium fallback for 'cloud engineer'\n",
      "2026-02-19 09:37:33,558 [INFO] Indeed 'cloud engineer': 15 jobs\n",
      "2026-02-19 09:37:33,570 [INFO] Saved: data\\indeed\\cloud_engineer.json\n",
      "2026-02-19 09:37:43,229 [INFO] Indeed: Searching 'java developer'\n",
      "2026-02-19 09:37:44,578 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:37:44,580 [INFO] Indeed Selenium fallback for 'java developer'\n",
      "2026-02-19 09:38:00,184 [INFO] Indeed 'java developer': 15 jobs\n",
      "2026-02-19 09:38:00,193 [INFO] Saved: data\\indeed\\java_developer.json\n",
      "2026-02-19 09:38:09,544 [INFO] Indeed: Searching 'react developer'\n",
      "2026-02-19 09:38:10,570 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:38:10,573 [INFO] Indeed Selenium fallback for 'react developer'\n",
      "2026-02-19 09:38:25,200 [INFO] Indeed 'react developer': 15 jobs\n",
      "2026-02-19 09:38:25,212 [INFO] Saved: data\\indeed\\react_developer.json\n",
      "2026-02-19 09:38:31,841 [INFO] Indeed: Searching 'nodejs developer'\n",
      "2026-02-19 09:38:33,324 [WARNING] Indeed blocked! Switching to Selenium...\n",
      "2026-02-19 09:38:33,328 [INFO] Indeed Selenium fallback for 'nodejs developer'\n",
      "2026-02-19 09:38:50,621 [INFO] Indeed 'nodejs developer': 15 jobs\n",
      "2026-02-19 09:38:50,624 [INFO] Saved: data\\indeed\\nodejs_developer.json\n",
      "2026-02-19 09:38:58,215 [INFO] Saved: data\\indeed\\all_indeed_jobs.json\n",
      "2026-02-19 09:38:58,218 [INFO] Indeed Total: 210 jobs\n",
      "2026-02-19 09:38:58,233 [INFO] \n",
      "==================================================\n",
      "2026-02-19 09:38:58,236 [INFO] STEP 3: LinkedIn Jobs\n",
      "2026-02-19 09:38:58,239 [INFO] ==================================================\n",
      "2026-02-19 09:38:58,243 [INFO] === LinkedIn Scraper Start ===\n",
      "2026-02-19 09:39:00,365 [INFO] LinkedIn: Searching 'software engineer'\n",
      "2026-02-19 09:39:06,494 [INFO] No more results for 'software engineer'\n",
      "2026-02-19 09:39:06,496 [INFO] LinkedIn 'software engineer': 0 jobs scraped\n",
      "2026-02-19 09:39:06,502 [INFO] Saved: data\\linkedin\\software_engineer.json\n",
      "2026-02-19 09:39:11,684 [INFO] LinkedIn: Searching 'data scientist'\n",
      "2026-02-19 09:39:18,009 [INFO] No more results for 'data scientist'\n",
      "2026-02-19 09:39:18,012 [INFO] LinkedIn 'data scientist': 0 jobs scraped\n",
      "2026-02-19 09:39:18,021 [INFO] Saved: data\\linkedin\\data_scientist.json\n",
      "2026-02-19 09:39:25,594 [INFO] LinkedIn: Searching 'machine learning engineer'\n",
      "2026-02-19 09:39:32,751 [INFO] No more results for 'machine learning engineer'\n",
      "2026-02-19 09:39:32,753 [INFO] LinkedIn 'machine learning engineer': 0 jobs scraped\n",
      "2026-02-19 09:39:32,761 [INFO] Saved: data\\linkedin\\machine_learning_engineer.json\n",
      "2026-02-19 09:39:42,102 [INFO] LinkedIn: Searching 'python developer'\n",
      "2026-02-19 09:39:48,385 [INFO] No more results for 'python developer'\n",
      "2026-02-19 09:39:48,387 [INFO] LinkedIn 'python developer': 0 jobs scraped\n",
      "2026-02-19 09:39:48,405 [INFO] Saved: data\\linkedin\\python_developer.json\n",
      "2026-02-19 09:39:55,092 [INFO] LinkedIn: Searching 'backend developer'\n",
      "2026-02-19 09:40:02,120 [INFO] No more results for 'backend developer'\n",
      "2026-02-19 09:40:02,123 [INFO] LinkedIn 'backend developer': 0 jobs scraped\n",
      "2026-02-19 09:40:02,134 [INFO] Saved: data\\linkedin\\backend_developer.json\n",
      "2026-02-19 09:40:11,430 [INFO] LinkedIn: Searching 'frontend developer'\n",
      "2026-02-19 09:40:18,613 [INFO] No more results for 'frontend developer'\n",
      "2026-02-19 09:40:18,617 [INFO] LinkedIn 'frontend developer': 0 jobs scraped\n",
      "2026-02-19 09:40:18,623 [INFO] Saved: data\\linkedin\\frontend_developer.json\n",
      "2026-02-19 09:40:25,522 [INFO] LinkedIn: Searching 'devops engineer'\n",
      "2026-02-19 09:40:32,700 [INFO] No more results for 'devops engineer'\n",
      "2026-02-19 09:40:32,703 [INFO] LinkedIn 'devops engineer': 0 jobs scraped\n",
      "2026-02-19 09:40:32,707 [INFO] Saved: data\\linkedin\\devops_engineer.json\n",
      "2026-02-19 09:40:39,291 [INFO] LinkedIn: Searching 'data analyst'\n",
      "2026-02-19 09:40:45,201 [INFO] No more results for 'data analyst'\n",
      "2026-02-19 09:40:45,202 [INFO] LinkedIn 'data analyst': 0 jobs scraped\n",
      "2026-02-19 09:40:45,208 [INFO] Saved: data\\linkedin\\data_analyst.json\n",
      "2026-02-19 09:40:53,706 [INFO] LinkedIn: Searching 'full stack developer'\n",
      "2026-02-19 09:41:01,094 [INFO] No more results for 'full stack developer'\n",
      "2026-02-19 09:41:01,102 [INFO] LinkedIn 'full stack developer': 0 jobs scraped\n",
      "2026-02-19 09:41:01,105 [INFO] Saved: data\\linkedin\\full_stack_developer.json\n",
      "2026-02-19 09:41:08,273 [INFO] LinkedIn: Searching 'AI engineer'\n",
      "2026-02-19 09:41:12,815 [INFO] No more results for 'AI engineer'\n",
      "2026-02-19 09:41:12,824 [INFO] LinkedIn 'AI engineer': 0 jobs scraped\n",
      "2026-02-19 09:41:12,832 [INFO] Saved: data\\linkedin\\AI_engineer.json\n",
      "2026-02-19 09:41:20,981 [INFO] Saved: data\\linkedin\\all_linkedin_jobs.json\n",
      "2026-02-19 09:41:20,984 [INFO] LinkedIn Total: 0 jobs\n",
      "2026-02-19 09:41:20,994 [INFO] Saved: data\\scraping_summary.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ╔══════════════════════════════════════════╗\n",
      "    ║              SCRAPING DONE!             ║\n",
      "    ╠══════════════════════════════════════════╣\n",
      "    ║  LinkedIn Jobs  : 0      records         ║\n",
      "    ║  Indeed Jobs    : 210    records         ║\n",
      "    ║  Kaggle Data    : 0      records         ║\n",
      "    ║  TOTAL          : 210    records         ║\n",
      "    ╚══════════════════════════════════════════╝\n",
      "\n",
      "    Data saved in: ./data/\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Resume Screening AI - Data Scraper\n",
    "Sources: LinkedIn Jobs, Indeed Jobs, Kaggle Datasets\n",
    "Output: JSON files in /data folder\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# ===================== SETUP =====================\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s'\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# Output folders\n",
    "BASE_DIR = Path(\"data\")\n",
    "LINKEDIN_DIR = BASE_DIR / \"linkedin\"\n",
    "INDEED_DIR = BASE_DIR / \"indeed\"\n",
    "KAGGLE_DIR = BASE_DIR / \"kaggle\"\n",
    "\n",
    "for d in [LINKEDIN_DIR, INDEED_DIR, KAGGLE_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Headers — real browser jaisi lagni chahiye\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "}\n",
    "\n",
    "def random_delay(min_sec=2, max_sec=5):\n",
    "    \"\"\"Ban hone se bachne ke liye random delay\"\"\"\n",
    "    time.sleep(random.uniform(min_sec, max_sec))\n",
    "\n",
    "def save_json(data, filepath):\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "    log.info(f\"Saved: {filepath}\")\n",
    "\n",
    "\n",
    "# ===================== 1. LINKEDIN SCRAPER =====================\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "def get_driver(headless=True):\n",
    "    \"\"\"Chrome driver setup\"\"\"\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "    opts.add_argument(\"--no-sandbox\")\n",
    "    opts.add_argument(\"--disable-dev-shm-usage\")\n",
    "    opts.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    opts.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    opts.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    opts.add_argument(f\"user-agent={HEADERS['User-Agent']}\")\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    return driver\n",
    "\n",
    "\n",
    "LINKEDIN_JOBS = [\n",
    "    \"software engineer\",\n",
    "    \"data scientist\",\n",
    "    \"machine learning engineer\",\n",
    "    \"python developer\",\n",
    "    \"backend developer\",\n",
    "    \"frontend developer\",\n",
    "    \"devops engineer\",\n",
    "    \"data analyst\",\n",
    "    \"full stack developer\",\n",
    "    \"AI engineer\",\n",
    "]\n",
    "\n",
    "def scrape_linkedin(max_per_keyword=100):\n",
    "    \"\"\"\n",
    "    LinkedIn public job listings scrape karta hai (login nahi chahiye)\n",
    "    \"\"\"\n",
    "    log.info(\"=== LinkedIn Scraper Start ===\")\n",
    "    driver = get_driver(headless=True)\n",
    "    all_jobs = []\n",
    "    \n",
    "    try:\n",
    "        for keyword in LINKEDIN_JOBS:\n",
    "            log.info(f\"LinkedIn: Searching '{keyword}'\")\n",
    "            keyword_jobs = []\n",
    "            start = 0\n",
    "            \n",
    "            while len(keyword_jobs) < max_per_keyword:\n",
    "                url = (\n",
    "                    f\"https://www.linkedin.com/jobs/search?\"\n",
    "                    f\"keywords={keyword.replace(' ', '%20')}\"\n",
    "                    f\"&location=&start={start}\"\n",
    "                )\n",
    "                \n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    random_delay(3, 6)\n",
    "                    \n",
    "                    # Job cards load hone ka wait\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.CLASS_NAME, \"jobs-search__results-list\"))\n",
    "                    )\n",
    "                    \n",
    "                    job_cards = driver.find_elements(By.CSS_SELECTOR, \"li.jobs-search__results-list > div\")\n",
    "                    \n",
    "                    if not job_cards:\n",
    "                        log.info(f\"No more results for '{keyword}'\")\n",
    "                        break\n",
    "                    \n",
    "                    for card in job_cards:\n",
    "                        try:\n",
    "                            job = {}\n",
    "                            \n",
    "                            # Title\n",
    "                            try:\n",
    "                                job[\"title\"] = card.find_element(\n",
    "                                    By.CSS_SELECTOR, \"h3.base-search-card__title\"\n",
    "                                ).text.strip()\n",
    "                            except NoSuchElementException:\n",
    "                                job[\"title\"] = \"\"\n",
    "                            \n",
    "                            # Company\n",
    "                            try:\n",
    "                                job[\"company\"] = card.find_element(\n",
    "                                    By.CSS_SELECTOR, \"h4.base-search-card__subtitle\"\n",
    "                                ).text.strip()\n",
    "                            except NoSuchElementException:\n",
    "                                job[\"company\"] = \"\"\n",
    "                            \n",
    "                            # Location\n",
    "                            try:\n",
    "                                job[\"location\"] = card.find_element(\n",
    "                                    By.CSS_SELECTOR, \"span.job-search-card__location\"\n",
    "                                ).text.strip()\n",
    "                            except NoSuchElementException:\n",
    "                                job[\"location\"] = \"\"\n",
    "                            \n",
    "                            # Job URL\n",
    "                            try:\n",
    "                                job[\"url\"] = card.find_element(\n",
    "                                    By.CSS_SELECTOR, \"a.base-card__full-link\"\n",
    "                                ).get_attribute(\"href\")\n",
    "                            except NoSuchElementException:\n",
    "                                job[\"url\"] = \"\"\n",
    "                            \n",
    "                            # Posted date\n",
    "                            try:\n",
    "                                job[\"posted\"] = card.find_element(\n",
    "                                    By.CSS_SELECTOR, \"time\"\n",
    "                                ).get_attribute(\"datetime\")\n",
    "                            except NoSuchElementException:\n",
    "                                job[\"posted\"] = \"\"\n",
    "                            \n",
    "                            job[\"keyword\"] = keyword\n",
    "                            job[\"source\"] = \"linkedin\"\n",
    "                            job[\"scraped_at\"] = datetime.now().isoformat()\n",
    "                            \n",
    "                            if job[\"title\"]:  # sirf valid jobs save karo\n",
    "                                keyword_jobs.append(job)\n",
    "                                \n",
    "                        except Exception as e:\n",
    "                            log.warning(f\"Card parse error: {e}\")\n",
    "                            continue\n",
    "                    \n",
    "                    # Job detail page se description bhi lo\n",
    "                    keyword_jobs = _linkedin_get_descriptions(driver, keyword_jobs)\n",
    "                    \n",
    "                    start += 25  # LinkedIn ka pagination\n",
    "                    random_delay(2, 4)\n",
    "                    \n",
    "                except TimeoutException:\n",
    "                    log.warning(f\"Timeout for keyword '{keyword}', moving on\")\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    log.error(f\"LinkedIn error: {e}\")\n",
    "                    break\n",
    "            \n",
    "            all_jobs.extend(keyword_jobs)\n",
    "            log.info(f\"LinkedIn '{keyword}': {len(keyword_jobs)} jobs scraped\")\n",
    "            \n",
    "            # Save per keyword\n",
    "            fname = LINKEDIN_DIR / f\"{keyword.replace(' ', '_')}.json\"\n",
    "            save_json(keyword_jobs, fname)\n",
    "            \n",
    "            random_delay(5, 10)  # keywords ke beech zyada wait\n",
    "    \n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    # All jobs ek file mein bhi\n",
    "    save_json(all_jobs, LINKEDIN_DIR / \"all_linkedin_jobs.json\")\n",
    "    log.info(f\"LinkedIn Total: {len(all_jobs)} jobs\")\n",
    "    return all_jobs\n",
    "\n",
    "\n",
    "def _linkedin_get_descriptions(driver, jobs, max_desc=20):\n",
    "    \"\"\"Job detail pages se description fetch karo\"\"\"\n",
    "    count = 0\n",
    "    for job in jobs:\n",
    "        if count >= max_desc:\n",
    "            break\n",
    "        if not job.get(\"url\") or job.get(\"description\"):\n",
    "            continue\n",
    "        try:\n",
    "            driver.get(job[\"url\"])\n",
    "            random_delay(2, 4)\n",
    "            desc_el = driver.find_element(\n",
    "                By.CSS_SELECTOR, \"div.show-more-less-html__markup\"\n",
    "            )\n",
    "            job[\"description\"] = desc_el.text.strip()\n",
    "            \n",
    "            # Requirements extract karne ki koshish\n",
    "            job[\"requirements\"] = _extract_requirements(job[\"description\"])\n",
    "            count += 1\n",
    "        except Exception:\n",
    "            job[\"description\"] = \"\"\n",
    "    return jobs\n",
    "\n",
    "\n",
    "def _extract_requirements(text):\n",
    "    \"\"\"Description se requirements section nikalo\"\"\"\n",
    "    requirements = []\n",
    "    lines = text.split(\"\\n\")\n",
    "    in_req = False\n",
    "    \n",
    "    req_keywords = [\"requirement\", \"qualifications\", \"what you need\", \"skills\", \"you have\"]\n",
    "    \n",
    "    for line in lines:\n",
    "        line_lower = line.lower()\n",
    "        if any(kw in line_lower for kw in req_keywords):\n",
    "            in_req = True\n",
    "        elif in_req and line.startswith(\"•\") or line.startswith(\"-\"):\n",
    "            requirements.append(line.strip(\"•- \").strip())\n",
    "        elif in_req and len(line) < 3:\n",
    "            in_req = False\n",
    "    \n",
    "    return requirements\n",
    "\n",
    "\n",
    "# ===================== 2. INDEED SCRAPER =====================\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "INDEED_JOBS = [\n",
    "    \"software engineer\",\n",
    "    \"data scientist\",\n",
    "    \"machine learning\",\n",
    "    \"python developer\",\n",
    "    \"backend developer\",\n",
    "    \"frontend developer\",\n",
    "    \"devops\",\n",
    "    \"data analyst\",\n",
    "    \"full stack developer\",\n",
    "    \"artificial intelligence\",\n",
    "    \"cloud engineer\",\n",
    "    \"java developer\",\n",
    "    \"react developer\",\n",
    "    \"nodejs developer\",\n",
    "]\n",
    "\n",
    "def scrape_indeed(max_per_keyword=200):\n",
    "    \"\"\"\n",
    "    Indeed se job listings scrape karta hai\n",
    "    \"\"\"\n",
    "    log.info(\"=== Indeed Scraper Start ===\")\n",
    "    session = requests.Session()\n",
    "    session.headers.update(HEADERS)\n",
    "    all_jobs = []\n",
    "    \n",
    "    for keyword in INDEED_JOBS:\n",
    "        log.info(f\"Indeed: Searching '{keyword}'\")\n",
    "        keyword_jobs = []\n",
    "        start = 0\n",
    "        \n",
    "        while len(keyword_jobs) < max_per_keyword:\n",
    "            url = (\n",
    "                f\"https://www.indeed.com/jobs?\"\n",
    "                f\"q={keyword.replace(' ', '+')}\"\n",
    "                f\"&start={start}\"\n",
    "                f\"&limit=50\"\n",
    "            )\n",
    "            \n",
    "            try:\n",
    "                resp = session.get(url, timeout=15)\n",
    "                \n",
    "                if resp.status_code == 403:\n",
    "                    log.warning(\"Indeed blocked! Switching to Selenium...\")\n",
    "                    keyword_jobs.extend(\n",
    "                        _indeed_selenium(keyword, max_per_keyword - len(keyword_jobs))\n",
    "                    )\n",
    "                    break\n",
    "                \n",
    "                if resp.status_code != 200:\n",
    "                    log.warning(f\"Indeed status: {resp.status_code}\")\n",
    "                    break\n",
    "                \n",
    "                soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "                \n",
    "                # Job cards\n",
    "                job_cards = soup.find_all(\"div\", class_=\"job_seen_beacon\")\n",
    "                \n",
    "                if not job_cards:\n",
    "                    # Alternative selectors try karo\n",
    "                    job_cards = soup.find_all(\"td\", class_=\"resultContent\")\n",
    "                \n",
    "                if not job_cards:\n",
    "                    log.info(f\"No more results for '{keyword}' at start={start}\")\n",
    "                    break\n",
    "                \n",
    "                for card in job_cards:\n",
    "                    try:\n",
    "                        job = {}\n",
    "                        \n",
    "                        # Title\n",
    "                        title_el = card.find(\"h2\", class_=\"jobTitle\")\n",
    "                        job[\"title\"] = title_el.get_text(strip=True) if title_el else \"\"\n",
    "                        \n",
    "                        # Company\n",
    "                        company_el = card.find(\"span\", {\"data-testid\": \"company-name\"})\n",
    "                        if not company_el:\n",
    "                            company_el = card.find(\"span\", class_=\"companyName\")\n",
    "                        job[\"company\"] = company_el.get_text(strip=True) if company_el else \"\"\n",
    "                        \n",
    "                        # Location\n",
    "                        loc_el = card.find(\"div\", {\"data-testid\": \"text-location\"})\n",
    "                        if not loc_el:\n",
    "                            loc_el = card.find(\"div\", class_=\"companyLocation\")\n",
    "                        job[\"location\"] = loc_el.get_text(strip=True) if loc_el else \"\"\n",
    "                        \n",
    "                        # Salary\n",
    "                        salary_el = card.find(\"div\", class_=\"metadata salary-snippet-container\")\n",
    "                        job[\"salary\"] = salary_el.get_text(strip=True) if salary_el else \"\"\n",
    "                        \n",
    "                        # Summary\n",
    "                        summary_el = card.find(\"div\", class_=\"job-snippet\")\n",
    "                        job[\"summary\"] = summary_el.get_text(strip=True) if summary_el else \"\"\n",
    "                        \n",
    "                        # Job link\n",
    "                        link_el = card.find(\"a\", class_=\"jcs-JobTitle\")\n",
    "                        if link_el:\n",
    "                            job[\"url\"] = \"https://www.indeed.com\" + link_el.get(\"href\", \"\")\n",
    "                            job[\"job_id\"] = link_el.get(\"data-jk\", \"\")\n",
    "                        else:\n",
    "                            job[\"url\"] = \"\"\n",
    "                            job[\"job_id\"] = \"\"\n",
    "                        \n",
    "                        job[\"keyword\"] = keyword\n",
    "                        job[\"source\"] = \"indeed\"\n",
    "                        job[\"scraped_at\"] = datetime.now().isoformat()\n",
    "                        \n",
    "                        if job[\"title\"]:\n",
    "                            keyword_jobs.append(job)\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        log.warning(f\"Indeed card error: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                start += 50\n",
    "                random_delay(3, 6)\n",
    "                \n",
    "            except requests.RequestException as e:\n",
    "                log.error(f\"Indeed request error: {e}\")\n",
    "                break\n",
    "        \n",
    "        # Job descriptions fetch karo (top 30 per keyword)\n",
    "        keyword_jobs = _indeed_get_descriptions(session, keyword_jobs, max_desc=30)\n",
    "        \n",
    "        all_jobs.extend(keyword_jobs)\n",
    "        log.info(f\"Indeed '{keyword}': {len(keyword_jobs)} jobs\")\n",
    "        \n",
    "        # Save per keyword\n",
    "        fname = INDEED_DIR / f\"{keyword.replace(' ', '_')}.json\"\n",
    "        save_json(keyword_jobs, fname)\n",
    "        \n",
    "        random_delay(5, 10)\n",
    "    \n",
    "    save_json(all_jobs, INDEED_DIR / \"all_indeed_jobs.json\")\n",
    "    log.info(f\"Indeed Total: {len(all_jobs)} jobs\")\n",
    "    return all_jobs\n",
    "\n",
    "\n",
    "def _indeed_get_descriptions(session, jobs, max_desc=30):\n",
    "    \"\"\"Indeed job detail pages se full description lo\"\"\"\n",
    "    count = 0\n",
    "    for job in jobs:\n",
    "        if count >= max_desc:\n",
    "            break\n",
    "        if not job.get(\"url\"):\n",
    "            continue\n",
    "        try:\n",
    "            resp = session.get(job[\"url\"], timeout=10)\n",
    "            if resp.status_code != 200:\n",
    "                continue\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            \n",
    "            desc_el = soup.find(\"div\", id=\"jobDescriptionText\")\n",
    "            if not desc_el:\n",
    "                desc_el = soup.find(\"div\", class_=\"jobsearch-jobDescriptionText\")\n",
    "            \n",
    "            if desc_el:\n",
    "                job[\"description\"] = desc_el.get_text(separator=\"\\n\", strip=True)\n",
    "                job[\"requirements\"] = _extract_requirements(job[\"description\"])\n",
    "            \n",
    "            count += 1\n",
    "            random_delay(1, 3)\n",
    "        \n",
    "        except Exception as e:\n",
    "            log.warning(f\"Indeed desc error: {e}\")\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "\n",
    "def _indeed_selenium(keyword, max_results=100):\n",
    "    \"\"\"Fallback: Selenium se Indeed scrape karo\"\"\"\n",
    "    log.info(f\"Indeed Selenium fallback for '{keyword}'\")\n",
    "    driver = get_driver(headless=True)\n",
    "    jobs = []\n",
    "    \n",
    "    try:\n",
    "        url = f\"https://www.indeed.com/jobs?q={keyword.replace(' ', '+')}\"\n",
    "        driver.get(url)\n",
    "        random_delay(3, 5)\n",
    "        \n",
    "        job_cards = driver.find_elements(By.CSS_SELECTOR, \"div.job_seen_beacon\")\n",
    "        \n",
    "        for card in job_cards[:max_results]:\n",
    "            try:\n",
    "                job = {}\n",
    "                try:\n",
    "                    job[\"title\"] = card.find_element(By.CSS_SELECTOR, \"h2.jobTitle\").text\n",
    "                except: job[\"title\"] = \"\"\n",
    "                try:\n",
    "                    job[\"company\"] = card.find_element(By.CSS_SELECTOR, \"[data-testid='company-name']\").text\n",
    "                except: job[\"company\"] = \"\"\n",
    "                try:\n",
    "                    job[\"location\"] = card.find_element(By.CSS_SELECTOR, \"[data-testid='text-location']\").text\n",
    "                except: job[\"location\"] = \"\"\n",
    "                \n",
    "                job[\"keyword\"] = keyword\n",
    "                job[\"source\"] = \"indeed_selenium\"\n",
    "                job[\"scraped_at\"] = datetime.now().isoformat()\n",
    "                \n",
    "                if job[\"title\"]:\n",
    "                    jobs.append(job)\n",
    "            except:\n",
    "                continue\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "\n",
    "# ===================== 3. KAGGLE DOWNLOADER =====================\n",
    "\n",
    "import subprocess\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Yeh datasets download karenge\n",
    "KAGGLE_DATASETS = [\n",
    "    {\n",
    "        \"id\": \"gauravduttakiit/resume-dataset\",\n",
    "        \"name\": \"resume_dataset\",\n",
    "        \"description\": \"2400+ categorized resumes\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"ravindrasinghrana/job-description-dataset\",\n",
    "        \"name\": \"job_descriptions\",\n",
    "        \"description\": \"Real job descriptions\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"arshkon/linkedin-job-postings\",\n",
    "        \"name\": \"linkedin_postings\",\n",
    "        \"description\": \"LinkedIn job postings 2023-2024\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"snehaanbhawal/resume-entities-for-ner\",\n",
    "        \"name\": \"resume_ner\",\n",
    "        \"description\": \"Resume entities for NER training\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"jillanisofttech/2023-it-professionals-resumes\",\n",
    "        \"name\": \"it_resumes_2023\",\n",
    "        \"description\": \"IT professional resumes\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def setup_kaggle_credentials():\n",
    "    \"\"\"\n",
    "    Kaggle credentials setup karo\n",
    "    ~/.kaggle/kaggle.json mein hona chahiye\n",
    "    \"\"\"\n",
    "    kaggle_dir = Path.home() / \".kaggle\"\n",
    "    kaggle_json = kaggle_dir / \"kaggle.json\"\n",
    "    \n",
    "    if kaggle_json.exists():\n",
    "        log.info(\"Kaggle credentials found!\")\n",
    "        return True\n",
    "    \n",
    "    log.warning(\"Kaggle credentials nahi mile!\")\n",
    "    log.info(\"\"\"\n",
    "    Kaggle setup karne ke liye:\n",
    "    1. kaggle.com pe account banao\n",
    "    2. Account Settings > API > Create New Token\n",
    "    3. kaggle.json download hoga\n",
    "    4. Yahan copy karo: ~/.kaggle/kaggle.json\n",
    "    5. chmod 600 ~/.kaggle/kaggle.json\n",
    "    \"\"\")\n",
    "    \n",
    "    # Manual input option\n",
    "    username = input(\"Kaggle username dalo (ya Enter skip ke liye): \").strip()\n",
    "    if username:\n",
    "        key = input(\"Kaggle API key dalo: \").strip()\n",
    "        kaggle_dir.mkdir(exist_ok=True)\n",
    "        creds = {\"username\": username, \"key\": key}\n",
    "        with open(kaggle_json, \"w\") as f:\n",
    "            json.dump(creds, f)\n",
    "        os.chmod(kaggle_json, 0o600)\n",
    "        log.info(\"Kaggle credentials save ho gaye!\")\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def download_kaggle_datasets():\n",
    "    \"\"\"Saare Kaggle datasets download karo aur JSON mein convert karo\"\"\"\n",
    "    log.info(\"=== Kaggle Downloader Start ===\")\n",
    "    \n",
    "    if not setup_kaggle_credentials():\n",
    "        log.error(\"Kaggle credentials nahi hain, skip kar raha hoon\")\n",
    "        return []\n",
    "    \n",
    "    # kaggle library install check\n",
    "    try:\n",
    "        import kaggle\n",
    "    except ImportError:\n",
    "        log.info(\"Kaggle installing...\")\n",
    "        subprocess.run([\"pip\", \"install\", \"kaggle\"], check=True)\n",
    "        import kaggle\n",
    "    \n",
    "    import pandas as pd\n",
    "    all_data = []\n",
    "    \n",
    "    for dataset in KAGGLE_DATASETS:\n",
    "        log.info(f\"Downloading: {dataset['name']} ({dataset['description']})\")\n",
    "        \n",
    "        download_path = KAGGLE_DIR / dataset[\"name\"]\n",
    "        download_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            # Download karo\n",
    "            subprocess.run([\n",
    "                \"kaggle\", \"datasets\", \"download\",\n",
    "                \"-d\", dataset[\"id\"],\n",
    "                \"-p\", str(download_path),\n",
    "                \"--unzip\"\n",
    "            ], check=True, capture_output=True)\n",
    "            \n",
    "            log.info(f\"Downloaded: {dataset['name']}\")\n",
    "            \n",
    "            # Files ko JSON mein convert karo\n",
    "            converted = _convert_to_json(download_path, dataset)\n",
    "            all_data.extend(converted)\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            log.error(f\"Download failed for {dataset['name']}: {e.stderr.decode()}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            log.error(f\"Error processing {dataset['name']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Master JSON\n",
    "    save_json(all_data, KAGGLE_DIR / \"all_kaggle_data.json\")\n",
    "    log.info(f\"Kaggle Total Records: {len(all_data)}\")\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def _convert_to_json(folder_path, dataset_info):\n",
    "    \"\"\"CSV/Excel files ko JSON mein convert karo\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    records = []\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    # Saari files dhundo\n",
    "    files = list(folder.rglob(\"*.csv\")) + \\\n",
    "            list(folder.rglob(\"*.xlsx\")) + \\\n",
    "            list(folder.rglob(\"*.json\"))\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            if file.suffix == \".csv\":\n",
    "                df = pd.read_csv(file, encoding=\"utf-8\", errors=\"replace\")\n",
    "            elif file.suffix == \".xlsx\":\n",
    "                df = pd.read_excel(file)\n",
    "            elif file.suffix == \".json\":\n",
    "                with open(file) as f:\n",
    "                    data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    records.extend(data)\n",
    "                else:\n",
    "                    records.append(data)\n",
    "                continue\n",
    "            \n",
    "            # DataFrame ko clean karo\n",
    "            df = df.dropna(how=\"all\")\n",
    "            df.columns = [c.lower().replace(\" \", \"_\") for c in df.columns]\n",
    "            \n",
    "            # JSON records\n",
    "            file_records = df.to_dict(orient=\"records\")\n",
    "            \n",
    "            # Metadata add karo\n",
    "            for rec in file_records:\n",
    "                rec[\"_source\"] = \"kaggle\"\n",
    "                rec[\"_dataset\"] = dataset_info[\"name\"]\n",
    "                rec[\"_file\"] = file.name\n",
    "                rec[\"_scraped_at\"] = datetime.now().isoformat()\n",
    "            \n",
    "            records.extend(file_records)\n",
    "            \n",
    "            # Per-file JSON save karo\n",
    "            output_file = folder_path / f\"{file.stem}_converted.json\"\n",
    "            save_json(file_records, output_file)\n",
    "            \n",
    "            log.info(f\"Converted {file.name}: {len(file_records)} records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            log.warning(f\"Could not convert {file}: {e}\")\n",
    "    \n",
    "    return records\n",
    "\n",
    "\n",
    "# ===================== 4. MAIN RUNNER =====================\n",
    "\n",
    "def run_all_scrapers():\n",
    "    \"\"\"Saare scrapers ek saath chalao\"\"\"\n",
    "    \n",
    "    print(\"\"\"\n",
    "    ╔══════════════════════════════════════════╗\n",
    "    ║   Resume Screening AI - Data Scraper    ║\n",
    "    ║   Sources: LinkedIn + Indeed + Kaggle   ║\n",
    "    ╚══════════════════════════════════════════╝\n",
    "    \"\"\")\n",
    "    \n",
    "    summary = {\n",
    "        \"started_at\": datetime.now().isoformat(),\n",
    "        \"linkedin\": 0,\n",
    "        \"indeed\": 0,\n",
    "        \"kaggle\": 0,\n",
    "        \"total\": 0\n",
    "    }\n",
    "    \n",
    "    # 1. Kaggle (pehle yeh karo - stable hai)\n",
    "    log.info(\"\\n\" + \"=\"*50)\n",
    "    log.info(\"STEP 1: Kaggle Datasets\")\n",
    "    log.info(\"=\"*50)\n",
    "    try:\n",
    "        kaggle_data = download_kaggle_datasets()\n",
    "        summary[\"kaggle\"] = len(kaggle_data)\n",
    "    except Exception as e:\n",
    "        log.error(f\"Kaggle failed: {e}\")\n",
    "    \n",
    "    # 2. Indeed\n",
    "    log.info(\"\\n\" + \"=\"*50)\n",
    "    log.info(\"STEP 2: Indeed Jobs\")\n",
    "    log.info(\"=\"*50)\n",
    "    try:\n",
    "        indeed_data = scrape_indeed(max_per_keyword=200)\n",
    "        summary[\"indeed\"] = len(indeed_data)\n",
    "    except Exception as e:\n",
    "        log.error(f\"Indeed failed: {e}\")\n",
    "    \n",
    "    # 3. LinkedIn\n",
    "    log.info(\"\\n\" + \"=\"*50)\n",
    "    log.info(\"STEP 3: LinkedIn Jobs\")\n",
    "    log.info(\"=\"*50)\n",
    "    try:\n",
    "        linkedin_data = scrape_linkedin(max_per_keyword=100)\n",
    "        summary[\"linkedin\"] = len(linkedin_data)\n",
    "    except Exception as e:\n",
    "        log.error(f\"LinkedIn failed: {e}\")\n",
    "    \n",
    "    # Summary\n",
    "    summary[\"total\"] = summary[\"linkedin\"] + summary[\"indeed\"] + summary[\"kaggle\"]\n",
    "    summary[\"completed_at\"] = datetime.now().isoformat()\n",
    "    \n",
    "    save_json(summary, BASE_DIR / \"scraping_summary.json\")\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    ╔══════════════════════════════════════════╗\n",
    "    ║              SCRAPING DONE!             ║\n",
    "    ╠══════════════════════════════════════════╣\n",
    "    ║  LinkedIn Jobs  : {summary['linkedin']:<6} records         ║\n",
    "    ║  Indeed Jobs    : {summary['indeed']:<6} records         ║\n",
    "    ║  Kaggle Data    : {summary['kaggle']:<6} records         ║\n",
    "    ║  TOTAL          : {summary['total']:<6} records         ║\n",
    "    ╚══════════════════════════════════════════╝\n",
    "    \n",
    "    Data saved in: ./data/\n",
    "    \"\"\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all_scrapers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d352a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Kaggle credentials save ho gaye!\n",
      "  File: C:\\Users\\HP\\.kaggle\\kaggle.json\n",
      "  Username: kalerii02\n",
      "✓ Kaggle API working hai!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ad5b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
